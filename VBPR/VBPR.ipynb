{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U fashion-clip\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h-and-m-personalized-fashion-recommendations.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 캐글 계정 -> 셋팅 -> 토큰생성 -> kaggle.json 파일 다운로드 됨\n",
    "# root/.kaggle 폴더 생성 후, kaggle.json 파일을 .kaggle 폴더로 이동\n",
    "# kaggle.json 파일 내부에 적혀있는 username과 key를 아래 코드에 넣어 설정 후 H&M 데이터 압축파일 다운로드\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"lijm1358\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"4cad47e5a4e3f3512be8b72f3b4dcdea\"\n",
    "\n",
    "!kaggle competitions download -c h-and-m-personalized-fashion-recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data # 데이터 폴더 생성\n",
    "!pwd # 현재 경로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 경로에 압축 풀고 싶다면 -d 이후에 원하는 경로로 변경\n",
    "!unzip -q h-and-m-personalized-fashion-recommendations.zip -d ./data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "item_data = pd.read_csv(\"../data/articles.csv\")\n",
    "interaction_data = pd.read_csv(\"../data/transactions_train.csv\")\n",
    "# user_data = pd.read_csv(\"./data/customers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### img prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "def img_by_id(df, article_id:int, no_list:list, echo:int=1, img_show:bool=True):\n",
    "    '''\n",
    "    article_id를 입력으로 받아 결과를 출력하는 함수\n",
    "    echo==1, 해당 아이템의 df row 출력\n",
    "    img_show=True, 해당 아이템의 이미지 출력\n",
    "    '''\n",
    "    if article_id in no_list:\n",
    "        return\n",
    "    if echo:\n",
    "        display(df[df.article_id == article_id])\n",
    "\n",
    "    img_id = \"0\"+str(article_id)\n",
    "    img = Image.open(\"../data/images/\"+img_id[0:3]+\"/\"+img_id+\".jpg\")\n",
    "\n",
    "    if img_show:\n",
    "        img.show()\n",
    "\n",
    "def find_no_img_item(df):\n",
    "    '''\n",
    "    이미지가 없는 아이템 찾아내는 함수\n",
    "    '''\n",
    "    no_img = []\n",
    "\n",
    "    for item in tqdm(df.iterrows(), total=len(df)):\n",
    "        try:\n",
    "            img_by_id(df, item[1][0], no_list=no_img, echo=0, img_show=False)\n",
    "        except FileNotFoundError:\n",
    "            no_img.append(item[0])\n",
    "\n",
    "    return no_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/105542 [00:00<?, ?it/s]/tmp/ipykernel_2304/3005365779.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  img_by_id(df, item[1][0], no_list=no_img, echo=0, img_show=False)\n",
      "100%|██████████| 105542/105542 [01:20<00:00, 1307.05it/s]\n"
     ]
    }
   ],
   "source": [
    "no_img_ids = find_no_img_item(item_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지가 없는 아이템을 구매 데이터와 아이템 데이터에서 삭제\n",
    "no_img_article_id = [item_data.iloc[x].article_id for x in no_img_ids]\n",
    "n_item_data = item_data.drop(no_img_ids, axis=0).reset_index(drop=True)\n",
    "n_interaction_data = interaction_data[~interaction_data[\"article_id\"].isin(no_img_article_id)].reset_index(drop=True)\n",
    "\n",
    "# train/test split을 위해 이력이 3초과인 유저만 남김\n",
    "n_interaction_data = n_interaction_data.groupby('customer_id').filter(lambda x: len(x) > 3).reset_index(drop=True)\n",
    "\n",
    "# (user/item)id를 index 맵핑\n",
    "user2idx = {v:k for k,v in enumerate(n_interaction_data['customer_id'].unique())}\n",
    "item2idx = {v:k for k,v in enumerate(n_item_data['article_id'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item2idx) # 전체 아이템 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models import alexnet, AlexNet_Weights, resnet18, ResNet18_Weights, vgg16, VGG16_Weights\n",
    "from fashion_clip.fashion_clip import FashionCLIP\n",
    "\n",
    "\n",
    "# # load pretrained alexnet\n",
    "# model_alex = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "# model_res = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "# model_vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# # del last clf layer\n",
    "# model_alex.classifier = model_alex.classifier[:-3]\n",
    "# model_res.fc = nn.Identity()\n",
    "# model_vgg.classifier = model_vgg.classifier[:-3]\n",
    "\n",
    "fclip = FashionCLIP('fashion-clip')\n",
    "\n",
    "# images = [\"./data/images/\" + \"0\" + str(k)[0:2] + \"/\" + \"0\"+str(k) + \".jpg\" for k in n_item_data[\"article_id\"].tolist()]\n",
    "# 패션 clip을 통한 이미지 임베딩 얻기\n",
    "# image_embeddings = fclip.encode_images(images, batch_size=32) \n",
    "# img_emb = torch.tensor(image_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "# feat_map_res = make_feature_map(model_res,n_item_data, book2idx)\n",
    "# feat_map_alex = make_feature_map(model_alex, n_book_data, book2idx)\n",
    "# feat_map_vgg = make_feature_map(model_vgg,n_book_data, book2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만들어둔 임베딩 csv로 저장해두기\n",
    "# pd.DataFrame(images).to_csv(\"img_list.csv\", index=False)\n",
    "# pd.DataFrame(image_embeddings).to_csv(\"img_emb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 임베딩 csv를 사용하는 경우\n",
    "# img_list = pd.read_csv(\"img_list.csv\")\n",
    "img_emb = pd.read_csv(\"./data/img_emb.csv\")\n",
    "img_emb = torch.tensor(img_emb.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img 16=\"0118458003\", 17=\"0118458004\"\n",
    "# 임베딩 잘 되었는지 확인하기\n",
    "res = nn.functional.cosine_similarity(img_emb[16], img_emb[17], dim=0)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class HMDataset(Dataset):\n",
    "    def __init__(self, df, user2idx, item2idx, is_train:bool=True) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.is_train = is_train\n",
    "        self.user2idx = user2idx\n",
    "        self.item2idx = item2idx\n",
    "        self.n_user = len(self.user2idx)\n",
    "        self.n_item = len(self.item2idx)\n",
    "        # mapping id2idx\n",
    "        self.df['article_id'] = self.df['article_id'].map(self.item2idx)\n",
    "        self.df['customer_id'] = self.df['customer_id'].map(self.user2idx)\n",
    "        \n",
    "        # train 데이터인 경우에만 neg 아이템이 생성\n",
    "        if is_train:\n",
    "            self.df['neg'] = np.zeros(len(self.df), dtype=int)\n",
    "            self._make_triples_data()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        user = self.df.customer_id[index]\n",
    "        pos = self.df.article_id[index]\n",
    "        \n",
    "        if self.is_train:\n",
    "            neg = self.df.neg[index]\n",
    "            return user, pos, neg\n",
    "        \n",
    "        return user, pos\n",
    "    \n",
    "    def _neg_sampling(self, pos_list):\n",
    "        '''\n",
    "        사용된 아이템 리스트(pos_list)에 없는 아이템 하나를 negative sample로 추출\n",
    "        '''\n",
    "        neg = np.random.randint(0,self.n_item,1) \n",
    "        while neg in pos_list:\n",
    "            neg = np.random.randint(0,self.n_item,1) \n",
    "        return neg\n",
    "\n",
    "    def _make_triples_data(self):\n",
    "        for id in tqdm(range(self.n_user)):\n",
    "            user_df = self.df[self.df.customer_id==id] # 유저 한 명 선택 \n",
    "            pos_list = (user_df.article_id).tolist()   # 해당 유저가 사용한 아이템 모두 추출\n",
    "            for i in range(len(user_df)): # 유저의 모든 구매 이력에 neg sample을 추가해줌\n",
    "                idx = user_df.index[i] \n",
    "                self.df.at[idx, 'neg'] = self._neg_sampling(pos_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "class HMTestDataset(Dataset):\n",
    "    def __init__(self, df, user2idx, item2idx, train_df) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.train_df = train_df\n",
    "        self.user2idx = user2idx\n",
    "        self.item2idx = item2idx\n",
    "        self.n_user = len(self.user2idx)\n",
    "        self.n_item = len(self.item2idx)\n",
    "        # mapping id2idx\n",
    "        self.df['article_id'] = self.df['article_id'].map(self.item2idx)\n",
    "        self.df['customer_id'] = self.df['customer_id'].map(self.user2idx)\n",
    "        self.df['neg'] = np.zeros(len(self.df), dtype=int)\n",
    "        self._make_triples_data()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        user = self.df.customer_id[index]\n",
    "        pos = self.df.article_id[index]\n",
    "        neg = self.df.neg[index]\n",
    "        return user, pos, neg\n",
    "            \n",
    "    def _neg_sampling(self, pos_list):\n",
    "        '''\n",
    "        사용된 아이템 리스트(pos_list)에 없는 아이템 하나를 negative sample로 추출\n",
    "        '''\n",
    "        neg = np.random.randint(0,self.n_item,1) \n",
    "        while neg in pos_list:\n",
    "            neg = np.random.randint(0,self.n_item,1) \n",
    "        return neg\n",
    "\n",
    "    def _make_triples_data(self):\n",
    "        for idx, row in tqdm(self.df.iterrows(), total=len(self.df)):\n",
    "            user_id = row.customer_id\n",
    "            user_df = self.train_df[self.train_df.customer_id==user_id]\n",
    "            pos_list = (user_df.article_id).tolist()   # 해당 유저가 사용한 아이템 모두 추출\n",
    "            self.df.at[idx, 'neg'] = self._neg_sampling(pos_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = n_interaction_data.groupby('customer_id').nth(-1) # 가장 마지막 구매 이력만 추출\n",
    "train_df = n_interaction_data[~n_interaction_data.index.isin(test_df.index)] # test에 해당하지 않는 데이터 모두 추출\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_interaction_data.shape, test_df.shape, train_df.shape, test_df.shape[0]+train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 잘 되었는지 확인해보기\n",
    "test_df.iloc[0].customer_id\n",
    "display(test_df[test_df.customer_id == '0021da829b898f82269fc51feded4eac2129058ee95bd75bb1591e2eb14ecc79'])\n",
    "display(train_df[train_df.customer_id == '0021da829b898f82269fc51feded4eac2129058ee95bd75bb1591e2eb14ecc79'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = HMDataset(train_df, user2idx, item2idx)\n",
    "# test_dataset = HMDataset(test_df, user2idx, item2idx, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_pt(data, path):\n",
    "#     with open(path, \"wb\") as file:\n",
    "#         torch.save(data, file)\n",
    "\n",
    "# save_pt(train_dataset, \"./dataset/train_dataset.pt\")\n",
    "# save_pt(test_dataset, \"./dataset/test_dataset.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eval을 위한 testset 만드는 부분 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pt(data, path):\n",
    "    with open(path, \"wb\") as file:\n",
    "        torch.save(data, file)\n",
    "\n",
    "import torch\n",
    "train_dataset = torch.load(\"./dataset/train_dataset.pt\")\n",
    "test_dataset = HMTestDataset(test_df, user2idx, item2idx, train_dataset.df)\n",
    "save_pt(test_dataset, \"./dataset/new_test_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class VBPR(nn.Module):\n",
    "    def __init__(self, n_user, n_item, K, D, img_embedding) -> None:\n",
    "        super().__init__()\n",
    "        self.feat_map= img_embedding.float() # user * 512\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.K = K\n",
    "        self.D = D\n",
    "        self.F = self.feat_map.shape[1] \n",
    "\n",
    "        self.offset = nn.Parameter(torch.zeros(1))\n",
    "        self.user_bias = nn.Embedding(self.n_user,1) # user*1\n",
    "        self.item_bias = nn.Embedding(self.n_item,1) # item*1\n",
    "        self.vis_bias = nn.Embedding(self.F,1)       # 512*1\n",
    "        self.user_emb = nn.Embedding(self.n_user,self.K) # user*K\n",
    "        self.item_emb = nn.Embedding(self.n_item,self.K) # item*K\n",
    "        self.item_vis_emb = nn.Embedding(self.D, self.F) # D*K\n",
    "        self.user_vis_emb = nn.Embedding(self.n_user, self.D) # user*D\n",
    "    \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.user_bias.weight)\n",
    "        nn.init.xavier_uniform_(self.item_bias.weight.data)\n",
    "        nn.init.xavier_uniform_(self.vis_bias.weight.data)\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight.data)\n",
    "        nn.init.xavier_uniform_(self.item_emb.weight.data)\n",
    "        nn.init.xavier_uniform_(self.item_vis_emb.weight.data)\n",
    "        nn.init.xavier_uniform_(self.user_vis_emb.weight.data)\n",
    "    \n",
    "    def cal_each(self, user, item):\n",
    "        vis_term = (self.user_vis_emb(user)@(self.item_vis_emb.weight@(self.feat_map[item].T))).sum(dim=1) + (self.vis_bias.weight.T)@(self.feat_map[item].T)\n",
    "        mf_term = self.offset + self.user_bias(user).T + self.item_bias(item).T + (self.user_emb(user)@self.item_emb(item).T).sum(dim=1).unsqueeze(dim=0)\n",
    "        params = (self.offset, self.user_bias(user), self.item_bias(item), self.vis_bias.weight, self.user_emb(user), self.item_emb(item), self.item_vis_emb.weight, self.user_vis_emb(user))\n",
    "        return (mf_term+vis_term).squeeze(), params\n",
    "    \n",
    "    def forward(self, user, pos, neg):\n",
    "        xui, pos_params = self.cal_each(user,pos)\n",
    "        xuj, neg_params = self.cal_each(user,neg)\n",
    "        return (xui-xuj), pos_params, neg_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self, reg_theta, reg_beta, reg_e) -> None:\n",
    "        super().__init__()\n",
    "        self.reg_theta = reg_theta\n",
    "        self.reg_beta = reg_beta\n",
    "        self.reg_e = reg_e\n",
    "\n",
    "    \n",
    "    def _cal_l2(self, *tensors):\n",
    "        total = 0\n",
    "        for tensor in tensors:\n",
    "            total += tensor.pow(2).sum()\n",
    "        return 0.5 * total\n",
    "\n",
    "    def _reg_term(self, pos_params, neg_params):\n",
    "        alpha, beta_u, beta_pos, beta_prime_pos, gamma_u, gamma_pos, e_pos, theta_u = pos_params\n",
    "        _, _, beta_neg, beta_prime_neg, _, gamma_neg, e_neg, _ = neg_params\n",
    "\n",
    "        reg_out = self.reg_theta * self._cal_l2(alpha, beta_u, beta_pos, beta_neg, theta_u, gamma_u, gamma_pos, gamma_neg)\n",
    "        reg_out += self.reg_beta * self._cal_l2(beta_prime_pos, beta_prime_neg)\n",
    "        reg_out += self.reg_e * self._cal_l2(e_pos, e_neg)\n",
    "\n",
    "        return reg_out\n",
    "\n",
    "    def forward(self, diff, pos_params, neg_params):\n",
    "        loss = -nn.functional.logsigmoid(diff).sum() # sigma(x_uij)\n",
    "        loss += self._reg_term(pos_params, neg_params) # reg_term\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for user, pos, neg in tqdm(dataloader):\n",
    "        user = user.to(device)\n",
    "        pos = pos.to(device)\n",
    "        neg = neg.to(device)\n",
    "\n",
    "        diff, pos_params, neg_params = model(user, pos, neg)\n",
    "        loss = criterion(diff, pos_params, neg_params)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "n_user = train_dataset.n_user\n",
    "n_item = train_dataset.n_item\n",
    "\n",
    "K = 20\n",
    "D = 20\n",
    "reg_theta = 0.1\n",
    "reg_beta = 0.1\n",
    "reg_e = 0\n",
    "\n",
    "lr = 0.001\n",
    "epoch = 10\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "criterion = BPRLoss(reg_theta, reg_beta, reg_e).to(device)\n",
    "img_emb = img_emb.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbpr_20 = VBPR(n_user, n_item, K, D, img_emb).to(device)\n",
    "optimizer = Adam(params = vbpr_20.parameters(), lr=lr)\n",
    "train_loss_20 = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    train_loss_20.append(train(vbpr_20, optimizer, train_dataloader, criterion, device))\n",
    "    print(f'EPOCH : {i} | LOSS : {train_loss_20[-1]:.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 40\n",
    "\n",
    "vbpr_40 = VBPR(n_user, n_item, K, D, img_emb).to(device)\n",
    "optimizer = Adam(params = vbpr_40.parameters(), lr=lr)\n",
    "train_loss_40 = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    train_loss_40.append(train(vbpr_40, optimizer, train_dataloader, criterion, device))\n",
    "    print(f'EPOCH : {i} | LOSS : {train_loss_40[-1]:.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 60\n",
    "\n",
    "vbpr_60 = VBPR(n_user, n_item, K, D, img_emb).to(device)\n",
    "optimizer = Adam(params = vbpr_60.parameters(), lr=lr)\n",
    "train_loss_60 = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    train_loss_60.append(train(vbpr_60, optimizer, train_dataloader, criterion, device))\n",
    "    print(f'EPOCH : {i} | LOSS : {train_loss_60[-1]:.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(epoch), train_loss_20, label=\"VBPR(factor 20)\")\n",
    "plt.plot(range(epoch), train_loss_40, label=\"VBPR(factor 40)\")\n",
    "plt.plot(range(epoch), train_loss_60, label=\"VBPR(factor 60)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top K rec test (해당 셀 이후부터는 아직 수정되지 않음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "class Recommender:\n",
    "    def __init__(self, model, query_img, train_dataset, n_item, img_emb, device) -> None:\n",
    "        self.model = model\n",
    "        self.train_df = train_dataset.dataset.df\n",
    "        self.all_item = set(range(0,n_item))\n",
    "        self.query_img = query_img\n",
    "        self.img_emb = img_emb\n",
    "        self.device = device\n",
    "\n",
    "    def _get_img_sim(self, itemset:list):\n",
    "        print(\"GET IMG SIM\")\n",
    "        res = []\n",
    "        for item in itemset:\n",
    "            res.append(nn.functional.cosine_similarity(self.query_img, self.img_emb[item.item()]))\n",
    "        return res\n",
    "\n",
    "    def _get_unobs_items(self, user_idx):\n",
    "        obs_item_set = set(self.train_df[self.train_df.user_id==user_idx].isbn)\n",
    "        return list(self.all_item - obs_item_set)\n",
    "\n",
    "    def user_rank(self, user_idx:int, top_k:int=None, img_sim_weight:float=0.5):\n",
    "        self.model.eval()\n",
    "        unobs_itemset = self._get_unobs_items(user_idx)\n",
    "        scaler = MaxAbsScaler()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            itemset = torch.tensor(unobs_itemset).to(self.device)\n",
    "            user = torch.tensor(np.full(len(itemset), user_idx)).to(self.device)\n",
    "            img_sim = torch.tensor(self._get_img_sim(itemset))\n",
    "\n",
    "            out, _ = self.model.cal_each(user, itemset)\n",
    "            out = scaler.fit_transform(out) # range [-1~1]\n",
    "            out = out + img_sim_weight*img_sim # range [-1.5~1.5]\n",
    "\n",
    "            scores = np.array(torch.concat((user.unsqueeze(dim=1),itemset.unsqueeze(dim=1),out.unsqueeze(dim=1)), dim=1))\n",
    "       \n",
    "        sorted_scores = scores[(-scores[:, 2]).argsort()]\n",
    "        return sorted_scores[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(recommender, test_dataset):\n",
    "    df = test_dataset.dataset.df\n",
    "    user_list = df['user_id'].unique()\n",
    "    res_true = {}\n",
    "    res_topk = {}\n",
    "    res_hit = {}\n",
    "    \n",
    "    for user in tqdm(df.iterrows(), total = len(df)):\n",
    "        res = recommender.user_rank(user, 20)\n",
    "        topk_item = res[:,1]\n",
    "        true_item = \n",
    "        hit = len(set(true_item).intersection(set(topk)))\n",
    "        res_true[user] = list(true_item)\n",
    "        res_topk[user] = list(topk)\n",
    "        res_hit[user] = hit\n",
    "    \n",
    "    return res_true, res_topk, res_hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = fclip.encode_images(query_img, batch_size=1)\n",
    "# res = img_sim(query, feat_map_vgg)\n",
    "# res\n",
    "recommender = Recommender(vbpr_20, query, train_dataset, n_item, img_emb, device)\n",
    "\n",
    "res_true, res_topk, res_hit = eval(recommender, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sequential",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
